{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting equivariant objects: Linear, TensorProducts, S2Grid, Equivariant Nonlinearities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivariant objects interact in more complex ways than invariant objects. Let's first consider multiplication. An invariant times an invariant produces another invariant. But, if interact two vectors via an outer product we get a $3\\times3$ matrix. Inside this matrix there is the trace (which is invariant), the antisymmetric part (which transforms like a vector), and the symmetric traceless part. This outer product operation is more generally known as a tensor product.\n",
    "\n",
    "In order to preserve equivariance, all operations we use must be equivariant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import e3nn\n",
    "from e3nn import o3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Products and their many forms\n",
    "Tensor products are the generalization of multiplication for tensor objects. Mathematically, the general tensor product is the product of two vector spaces $\\otimes: X, Y \\rightarrow X \\times Y$ . For geometric tensors, we can express the vector spaces $X$ and $Y$ in terms of irreducible representations, thus we can also express the product space in terms of a new space of irreducible representations $X \\times Y \\rightarrow Z$.\n",
    "\n",
    "In the general case, this product converves the number of degrees of freedom. For equivariant neural networks, we will include learnable parameters in the tensor product. We also will often omit parts of this product to reduce memory usage and improve computation speed. This is why our `o3.TensorProduct` class is more involved than a traditional tensor product. \n",
    "\n",
    "The `o3.TensorProduct` class is the workhorse of `e3nn`. It's very powerful and very flexible but can be challenging to grok. You will most often deal with subclasses of this class.\n",
    "\n",
    "Let's start with the more traditional tensor product which is articulated with `o3.FullTensorProduct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_product_two_vectors = o3.FullTensorProduct('1o', '1o')\n",
    "outer_product_two_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_product_two_vectors.instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = o3.FullTensorProduct('5x1o + 4x3e', '10x2e')\n",
    "tp.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have several types of prebuilt tensor products which use different \"instruction sets\". The FullTensorProduct is what is most commonly thought of as the general tensor product, as used in quantum mechanics (in the addition of angular momentum). However, general tensor products are combinatorial as you interact more and more objects, in order to keep the cost down, we will use ElementwiseTensorProducts and other instruction subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElementwiseTensorProduct\n",
    "With `o3.ElementwiseTensorProduct` we can pair up irreps across two inputs and only compute tensor products within those pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.ElementwiseTensorProduct('1x0e + 2x1o', '2x0e + 1x1o').visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## FullyConnectedTensorProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.FullyConnectedTensorProduct('1x0e + 2x1o', '2x0e + 1x1o', irreps_out='5x0e + 5x1e').visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Advanced] Using TensorProduct directly\n",
    "In most cases, you will use the above subclasses of `TensorProduct`. However, you can also have complete control over which \"paths\" you are including by using `TensorProduct`. We do NOT recommend this for the new user.\n",
    "\n",
    "For example, here is how to explicity re-write the above specific `o3.FullyConnectedTensorProduct` example with `o3.TensorProduct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.TensorProduct('1x0e + 2x1o', '2x0e + 1x1o', out='5x0e + 5x1e', \n",
    "                 instructions=[\n",
    "                     # input 1 1x0e \\otimes input 2 2x0e and apply weights to contribute to 5x0e output\n",
    "                     (0, 0, 0, 'uvw', True), \n",
    "                     # input 1 2x1o \\otimes input 2 1x1o and apply weights to contribute to 5x0e output\n",
    "                     (1, 1, 0, 'uvw', True),\n",
    "                     # input 1 2x1o \\otimes input 2 1x1o and apply weights to contribute to 5x1e output\n",
    "                     (1, 1, 1, 'uvw', True),\n",
    "                 ]).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling permutations of indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `o3.ReducedTensorProducts`, we can additionally restrict tensor products to obey specific permutation symmetries of the input tensors. For example, this is helpful for computing the power spectrum and bispectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e3nn import io\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lmax = 4\n",
    "p_val, p_arg = 1, -1  # parity of signal on sphere, parity of vector to sphere from origin\n",
    "\n",
    "sph = io.SphericalTensor(lmax, p_val, p_arg)\n",
    "peaks = torch.tensor([[1., 0., 0.], [-1., 0., 0.]])\n",
    "signal = sph.with_peaks_at(peaks)\n",
    "\n",
    "power_spectrum = o3.ReducedTensorProducts(\n",
    "    'ij=ji', i=o3.Irreps.spherical_harmonics(lmax), \n",
    "    set_ir_out=list(o3.Irrep.iterator(lmax=0)),  # Only want outputs that are scalar or pseudoscalars\n",
    "    set_ir_mid=list(o3.Irrep.iterator(lmax))  # Don't compute contributions with greater than lmax\n",
    ")\n",
    "power = lambda x: power_spectrum(x, x)\n",
    "\n",
    "bi_spectrum = o3.ReducedTensorProducts(\n",
    "    'ijk=jik=kji', i=o3.Irreps.spherical_harmonics(lmax), \n",
    "    set_ir_out=list(o3.Irrep.iterator(lmax=0)),  # Only want outputs that are scalar or pseudoscalars\n",
    "    set_ir_mid=list(o3.Irrep.iterator(lmax))  # Don't compute contributions with greater than lmax\n",
    ")\n",
    "bi = lambda x: bi_spectrum(x, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1)\n",
    "\n",
    "val = 0.1\n",
    "\n",
    "ax[0].set_title('Signal')\n",
    "ax[0].imshow(signal[None, :], cmap='RdBu', vmin=-val, vmax=val)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[0].set_xlabel(o3.Irreps.spherical_harmonics(lmax))\n",
    "\n",
    "ax[1].set_title('Power Spectrum')\n",
    "ax[1].imshow(power(signal)[None, :], cmap='RdBu', vmin=-val, vmax=val)\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xlabel(power_spectrum.irreps_out);\n",
    "\n",
    "ax[2].set_title('Bispectrum')\n",
    "ax[2].imshow(bi(signal)[None, :], cmap='RdBu', vmin=-val, vmax=val)\n",
    "ax[2].set_xticks([])\n",
    "ax[2].set_yticks([])\n",
    "ax[2].set_xlabel(bi_spectrum.irreps_out);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layers\n",
    "Equivariant Linear layers are able to mix channels between the same irreps, but not across irreps. Linear layers are actually built with tensor products -- a scalar of `1.0` stands in as the second input. Weight matrices are contrained in the `o3.Linear.tp` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irreps_in = '5x0e + 6x1o'\n",
    "irreps_out = '7x0e + 5x1o'\n",
    "linear = o3.Linear(irreps_in, irreps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.tp.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.tp.instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All learnable parameters must be scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to preserve equivariance, all learnable parameters in the model must be scalars. For $f: X, W \\rightarrow Y$,\n",
    "\n",
    "$f(D_X(g)x, w) = D_Y(g)f(x, w), \\text{ if } \\forall g \\in G, D_W(g) = I$\n",
    "\n",
    "For the irreps of $O(3)$, this condition is only met if $w$ is purely scalar.\n",
    "\n",
    "Otherwise, we would need to rotate our learnable features with our input data, which largely defeats the purpose of creating an equivariant neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
